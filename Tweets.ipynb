{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2234ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the ZIP file\n",
    "zip_path = r'C:\\Users\\USER\\Downloads\\Twitter-Sentiments.zip'\n",
    "# Directory where you want to extract the files\n",
    "extract_path = r'C:\\Users\\USER\\Downloads\\Twitter-Sentiments'\n",
    "\n",
    "# Extract the ZIP file\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_path)\n",
    "\n",
    "# Load the dataset (assuming it is a CSV file)\n",
    "csv_file_path = os.path.join(extract_path, 'Tweets.csv')\n",
    "tweets = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "tweets.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4086fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14640 entries, 0 to 14639\n",
      "Data columns (total 15 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   tweet_id                      14640 non-null  int64  \n",
      " 1   airline_sentiment             14640 non-null  object \n",
      " 2   airline_sentiment_confidence  14640 non-null  float64\n",
      " 3   negativereason                9178 non-null   object \n",
      " 4   negativereason_confidence     10522 non-null  float64\n",
      " 5   airline                       14640 non-null  object \n",
      " 6   airline_sentiment_gold        40 non-null     object \n",
      " 7   name                          14640 non-null  object \n",
      " 8   negativereason_gold           32 non-null     object \n",
      " 9   retweet_count                 14640 non-null  int64  \n",
      " 10  text                          14640 non-null  object \n",
      " 11  tweet_coord                   1019 non-null   object \n",
      " 12  tweet_created                 14640 non-null  object \n",
      " 13  tweet_location                9907 non-null   object \n",
      " 14  user_timezone                 9820 non-null   object \n",
      "dtypes: float64(2), int64(2), object(11)\n",
      "memory usage: 1.7+ MB\n",
      "None\n",
      "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
      "0  570306133677760513           neutral                        1.0000   \n",
      "1  570301130888122368          positive                        0.3486   \n",
      "2  570301083672813571           neutral                        0.6837   \n",
      "3  570301031407624196          negative                        1.0000   \n",
      "4  570300817074462722          negative                        1.0000   \n",
      "\n",
      "  negativereason  negativereason_confidence         airline  \\\n",
      "0            NaN                        NaN  Virgin America   \n",
      "1            NaN                     0.0000  Virgin America   \n",
      "2            NaN                        NaN  Virgin America   \n",
      "3     Bad Flight                     0.7033  Virgin America   \n",
      "4     Can't Tell                     1.0000  Virgin America   \n",
      "\n",
      "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
      "0                    NaN     cairdin                 NaN              0   \n",
      "1                    NaN    jnardino                 NaN              0   \n",
      "2                    NaN  yvonnalynn                 NaN              0   \n",
      "3                    NaN    jnardino                 NaN              0   \n",
      "4                    NaN    jnardino                 NaN              0   \n",
      "\n",
      "                                                text tweet_coord  \\\n",
      "0                @VirginAmerica What @dhepburn said.         NaN   \n",
      "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
      "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
      "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
      "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
      "\n",
      "               tweet_created tweet_location               user_timezone  \n",
      "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
      "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
      "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
      "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
      "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  \n",
      "tweet_id                            0\n",
      "airline_sentiment                   0\n",
      "airline_sentiment_confidence        0\n",
      "negativereason                   5462\n",
      "negativereason_confidence        4118\n",
      "airline                             0\n",
      "airline_sentiment_gold          14600\n",
      "name                                0\n",
      "negativereason_gold             14608\n",
      "retweet_count                       0\n",
      "text                                0\n",
      "tweet_coord                     13621\n",
      "tweet_created                       0\n",
      "tweet_location                   4733\n",
      "user_timezone                    4820\n",
      "dtype: int64\n",
      "  airline_sentiment  airline_sentiment_confidence negativereason  \\\n",
      "0           neutral                        1.0000            NaN   \n",
      "1          positive                        0.3486            NaN   \n",
      "2           neutral                        0.6837            NaN   \n",
      "3          negative                        1.0000     Bad Flight   \n",
      "4          negative                        1.0000     Can't Tell   \n",
      "\n",
      "   negativereason_confidence         airline        name  retweet_count  \\\n",
      "0                        NaN  Virgin America     cairdin              0   \n",
      "1                     0.0000  Virgin America    jnardino              0   \n",
      "2                        NaN  Virgin America  yvonnalynn              0   \n",
      "3                     0.7033  Virgin America    jnardino              0   \n",
      "4                     1.0000  Virgin America    jnardino              0   \n",
      "\n",
      "                                                text  \\\n",
      "0                @VirginAmerica What @dhepburn said.   \n",
      "1  @VirginAmerica plus you've added commercials t...   \n",
      "2  @VirginAmerica I didn't today... Must mean I n...   \n",
      "3  @VirginAmerica it's really aggressive to blast...   \n",
      "4  @VirginAmerica and it's a really big bad thing...   \n",
      "\n",
      "               tweet_created  \n",
      "0  2015-02-24 11:35:52 -0800  \n",
      "1  2015-02-24 11:15:59 -0800  \n",
      "2  2015-02-24 11:15:48 -0800  \n",
      "3  2015-02-24 11:15:36 -0800  \n",
      "4  2015-02-24 11:14:45 -0800  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(tweets.info())\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(tweets.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(tweets.isnull().sum())\n",
    "\n",
    "# Drop unnecessary columns\n",
    "tweets_cleaned = tweets.drop(columns=['tweet_id', 'airline_sentiment_gold', 'negativereason_gold', 'tweet_coord', 'tweet_location', 'user_timezone'])\n",
    "\n",
    "# Handle missing values\n",
    "# For simplicity, we'll drop rows with missing values in 'airline_sentiment' and 'text'\n",
    "tweets_cleaned = tweets_cleaned.dropna(subset=['airline_sentiment', 'text'])\n",
    "\n",
    "# Display the first few rows of the cleaned dataset\n",
    "print(tweets_cleaned.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e7d2943",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0                @VirginAmerica What @dhepburn said.   \n",
      "1  @VirginAmerica plus you've added commercials t...   \n",
      "2  @VirginAmerica I didn't today... Must mean I n...   \n",
      "3  @VirginAmerica it's really aggressive to blast...   \n",
      "4  @VirginAmerica and it's a really big bad thing...   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0                        virginamerica dhepburn said  \n",
      "1  virginamerica plus youve added commercial expe...  \n",
      "2  virginamerica didnt today must mean need take ...  \n",
      "3  virginamerica really aggressive blast obnoxiou...  \n",
      "4                 virginamerica really big bad thing  \n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Removing punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Removing stopwords and lemmatization\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stopwords.words('english')]\n",
    "    \n",
    "    # Join tokens back to a single string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply the preprocessing function to the 'text' column\n",
    "tweets_cleaned['cleaned_text'] = tweets_cleaned['text'].apply(preprocess_text)\n",
    "\n",
    "# Display the first few rows of the cleaned dataset\n",
    "print(tweets_cleaned[['text', 'cleaned_text']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbcf11a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14640, 5000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "# Fit and transform the cleaned text\n",
    "X = tfidf_vectorizer.fit_transform(tweets_cleaned['cleaned_text'])\n",
    "\n",
    "# Convert to DataFrame for better readability\n",
    "X = pd.DataFrame(X.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display the shape of the resulting feature matrix\n",
    "print(X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d10ba626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.94      0.86      1889\n",
      "     neutral       0.63      0.39      0.48       580\n",
      "    positive       0.75      0.53      0.62       459\n",
      "\n",
      "    accuracy                           0.77      2928\n",
      "   macro avg       0.72      0.62      0.66      2928\n",
      "weighted avg       0.75      0.77      0.75      2928\n",
      "\n",
      "Accuracy: 0.7670765027322405\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Target variable\n",
    "y = tweets_cleaned['airline_sentiment']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the Random Forest classifier\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affdcd33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 324 candidates, totalling 972 fits\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Initialize the Random Forest Classifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit the GridSearchCV to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best parameters found: \", best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d0d8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing notebook environment.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8775aa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef439c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15cc518",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879ffba3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101bfb1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
